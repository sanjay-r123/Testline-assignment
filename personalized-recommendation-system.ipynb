{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10578076,"sourceType":"datasetVersion","datasetId":6546214}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-01-25T13:24:51.545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple\nimport statistics\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport torch\nimport os\n\nclass StudentAnalyzer:\n    def __init__(self):\n        # Load data\n        with open('/kaggle/input/dataset/api endpoints.json', 'r') as f:\n            self.historical_data = json.load(f)\n        with open('/kaggle/input/dataset/quiz submission data.json', 'r') as f:\n            self.current_quiz = json.load(f)\n        with open('/kaggle/input/dataset/quiz endpoint.json', 'r') as f:\n            self.quiz_questions = json.load(f)\n            \n        # Initialize ML components for score prediction\n        self.score_predictor = LinearRegression()\n        self.scaler = StandardScaler()\n        \n        # Check for GPU availability\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {self.device}\")\n        \n        try:\n            # Initialize LLM pipeline\n            print(\"Loading LLM model...\")\n            token = \"use your own hugging case read token here\"\n            \n            self.pipe = pipeline(\n                \"text-generation\",\n                model=\"meta-llama/Llama-3.2-3B-Instruct\",\n                token=token,\n                torch_dtype=torch.float16,\n                device_map=\"auto\",\n            )\n            print(\"LLM model loaded successfully!\")\n        except Exception as e:\n            print(f\"Error loading LLM model: {str(e)}\")\n            raise\n\n    def _analyze_topic_performance(self) -> Dict[str, float]:\n        \"\"\"Analyze performance by topic\"\"\"\n        topic_scores = {}\n        \n        for quiz in self.historical_data[:5]:\n            topic = quiz['quiz']['topic']\n            accuracy = float(quiz['accuracy'].replace(' %', ''))\n            \n            if topic not in topic_scores:\n                topic_scores[topic] = []\n            topic_scores[topic].append(accuracy)\n            \n        # Calculate average accuracy per topic\n        return {topic: statistics.mean(scores) for topic, scores in topic_scores.items()}\n\n    def _identify_strengths_weaknesses(self, topic_performance: Dict[str, float]) -> Tuple[List[str], List[str]]:\n        \"\"\"Identify strong and weak topics\"\"\"\n        if not topic_performance:\n            return [], []\n            \n        avg = statistics.mean(topic_performance.values())\n        strengths = [topic for topic, score in topic_performance.items() if score >= avg + 5]\n        weaknesses = [topic for topic, score in topic_performance.items() if score <= avg - 5]\n        \n        return strengths, weaknesses\n\n    def _prepare_ml_data(self) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Prepare data for ML analysis\"\"\"\n        features = []\n        targets = []\n        \n        for quiz in self.historical_data:\n            # Extract features\n            feature_vector = [\n                float(quiz['accuracy'].replace(' %', '')),\n                float(quiz['speed']),\n                quiz['correct_answers'],\n                quiz['incorrect_answers'],\n                len(quiz['response_map']),  # questions attempted\n                quiz['mistakes_corrected']\n            ]\n            features.append(feature_vector)\n            targets.append(quiz['score'])\n            \n        return np.array(features), np.array(targets)\n\n    def predict_potential_score(self) -> float:\n        \"\"\"Predict potential score based on current performance patterns\"\"\"\n        X, y = self._prepare_ml_data()\n        \n        if len(X) < 2:  # Need at least 2 data points for meaningful prediction\n            return None\n            \n        # Scale features\n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Train model\n        self.score_predictor.fit(X_scaled, y)\n        \n        # Prepare current quiz features\n        current_features = np.array([[\n            float(self.current_quiz['accuracy'].replace(' %', '')),\n            float(self.current_quiz['speed']),\n            self.current_quiz['correct_answers'],\n            self.current_quiz['incorrect_answers'],\n            len(self.current_quiz['response_map']),\n            self.current_quiz['mistakes_corrected']\n        ]])\n        \n        # Scale and predict\n        current_scaled = self.scaler.transform(current_features)\n        potential_score = self.score_predictor.predict(current_scaled)[0]\n        \n        return potential_score\n\n    def analyze_question_patterns(self) -> Dict:\n        \"\"\"Analyze patterns in question responses\"\"\"\n        patterns = {\n            'time_based_errors': 0,\n            'conceptual_gaps': defaultdict(int),\n            'careless_mistakes': 0,\n            'topic_wise_accuracy': defaultdict(lambda: {'correct': 0, 'total': 0})\n        }\n        \n        # Analyze current quiz responses\n        for q_id, opt_id in self.current_quiz['response_map'].items():\n            # Find question in quiz_questions\n            question = next((q for q in self.quiz_questions['quiz']['questions'] \n                           if str(q['id']) == str(q_id)), None)\n            \n            if question:\n                topic = question['topic']\n                correct_option = next((opt for opt in question['options'] \n                                    if opt['is_correct']), None)\n                \n                # Update topic-wise accuracy\n                patterns['topic_wise_accuracy'][topic]['total'] += 1\n                if correct_option and str(correct_option['id']) == str(opt_id):\n                    patterns['topic_wise_accuracy'][topic]['correct'] += 1\n                else:\n                    # Analyze error patterns\n                    if float(self.current_quiz['speed']) > 95:\n                        patterns['time_based_errors'] += 1\n                    patterns['conceptual_gaps'][topic] += 1\n                    \n                    # Check for careless mistakes\n                    if self.current_quiz['mistakes_corrected'] > 0:\n                        patterns['careless_mistakes'] += 1\n        \n        return patterns\n\n    def calculate_learning_curve(self) -> Dict:\n        \"\"\"Calculate learning curve metrics\"\"\"\n        scores = [quiz['score'] for quiz in self.historical_data]\n        accuracies = [float(quiz['accuracy'].replace(' %', '')) \n                     for quiz in self.historical_data]\n        \n        if len(scores) < 2:\n            return None\n            \n        # Calculate improvement rate\n        score_improvement = (scores[0] - scores[-1]) / len(scores)\n        accuracy_improvement = (accuracies[0] - accuracies[-1]) / len(accuracies)\n        \n        return {\n            'score_trend': score_improvement,\n            'accuracy_trend': accuracy_improvement,\n            'consistency_score': np.std(scores),  # Lower is better\n            'learning_rate': abs(score_improvement / np.mean(scores)) * 100\n        }\n\n    def analyze_performance(self) -> Dict:\n        \"\"\"Enhanced performance analysis\"\"\"\n        # Calculate key metrics\n        recent_scores = [quiz['score'] for quiz in self.historical_data[:5]]\n        avg_score = statistics.mean(recent_scores) if recent_scores else 0\n        \n        accuracies = [float(quiz['accuracy'].replace(' %', '')) for quiz in self.historical_data[:5]]\n        avg_accuracy = statistics.mean(accuracies) if accuracies else 0\n        \n        speeds = [float(quiz['speed']) for quiz in self.historical_data[:5]]\n        avg_speed = statistics.mean(speeds) if speeds else 0\n        \n        # Get topic performance\n        topic_performance = self._analyze_topic_performance()\n        strengths, weaknesses = self._identify_strengths_weaknesses(topic_performance)\n        \n        analysis = {\n            'average_score': avg_score,\n            'average_accuracy': avg_accuracy,\n            'average_speed': avg_speed,\n            'topic_performance': topic_performance,\n            'strengths': strengths,\n            'weaknesses': weaknesses\n        }\n        \n        # Add ML predictions\n        potential_score = self.predict_potential_score()\n        if potential_score is not None:\n            analysis['potential_score'] = potential_score\n            analysis['score_gap'] = potential_score - analysis['average_score']\n        \n        # Add question patterns\n        analysis['question_patterns'] = self.analyze_question_patterns()\n        \n        # Add learning curve metrics\n        learning_metrics = self.calculate_learning_curve()\n        if learning_metrics:\n            analysis['learning_metrics'] = learning_metrics\n        \n        return analysis\n\n    def get_student_persona(self) -> Dict:\n        \"\"\"Enhanced student persona with ML insights\"\"\"\n        analysis = self.analyze_performance()\n        \n        # Determine base persona\n        if analysis['average_accuracy'] >= 85 and analysis['average_speed'] >= 95:\n            base_persona = \"Advanced Achiever\"\n        elif analysis['average_accuracy'] >= 75:\n            base_persona = \"Steady Performer\"\n        elif analysis['average_speed'] >= 90:\n            base_persona = \"Quick Learner\"\n        else:\n            base_persona = \"Building Foundations\"\n        \n        # Enhanced characteristics\n        characteristics = {\n            'learning_style': 'Consistent' if analysis.get('learning_metrics', {}).get('consistency_score', 100) < 15 \n                            else 'Variable',\n            'performance_potential': 'High' if analysis.get('score_gap', 0) > 15 \n                                   else 'Moderate' if analysis.get('score_gap', 0) > 5 \n                                   else 'Optimized',\n            'error_pattern': 'Time-pressured' if analysis['question_patterns']['time_based_errors'] > 3\n                           else 'Conceptual' if sum(analysis['question_patterns']['conceptual_gaps'].values()) > 5\n                           else 'Balanced'\n        }\n        \n        return {\n            'base_persona': base_persona,\n            'characteristics': characteristics\n        }\n\n    def generate_recommendations(self) -> Dict:\n        \"\"\"Enhanced recommendations with ML insights\"\"\"\n        analysis = self.analyze_performance()\n        recommendations = {\n            'focus_areas': [],\n            'study_tips': [],\n            'practice_suggestions': []\n        }\n        \n        # Add topic-specific recommendations\n        for topic in analysis['weaknesses']:\n            recommendations['focus_areas'].append(f\"Review core concepts in {topic}\")\n            recommendations['practice_suggestions'].append(f\"Take more practice quizzes on {topic}\")\n        \n        # Add ML-based recommendations\n        if 'potential_score' in analysis:\n            score_gap = analysis['score_gap']\n            if score_gap > 10:\n                recommendations['potential_improvement'] = [\n                    f\"You have the potential to improve your score by {score_gap:.1f} points\",\n                    \"Focus on reducing careless mistakes\",\n                    \"Review questions where you changed your answer\"\n                ]\n        \n        # Add pattern-based recommendations\n        patterns = analysis['question_patterns']\n        if patterns['time_based_errors'] > 0:\n            recommendations['study_tips'].append(\n                \"Consider spending more time on complex questions\"\n            )\n        \n        # Add topic-specific recommendations based on conceptual gaps\n        for topic, count in patterns['conceptual_gaps'].items():\n            if count > 2:\n                recommendations['focus_areas'].append(\n                    f\"Deep dive into fundamental concepts of {topic}\"\n                )\n        \n        # Add learning curve based recommendations\n        if 'learning_metrics' in analysis:\n            metrics = analysis['learning_metrics']\n            if metrics['consistency_score'] > 20:\n                recommendations['study_tips'].append(\n                    \"Work on maintaining consistent performance across quizzes\"\n                )\n            if metrics['learning_rate'] < 5:\n                recommendations['study_tips'].append(\n                    \"Consider revising your study strategy to improve learning rate\"\n                )\n        \n        return recommendations\n\n    def analyze_historical_trends(self) -> Dict:\n        \"\"\"Analyze trends from historical quiz data\"\"\"\n        last_5_quizzes = self.historical_data[:5]\n        \n        trends = {\n            'score_progression': [],\n            'accuracy_progression': [],\n            'speed_progression': [],\n            'topic_mastery': defaultdict(list),\n            'question_patterns': {\n                'response_time_trends': [],\n                'mistake_patterns': defaultdict(int),\n                'improvement_areas': set()\n            }\n        }\n        \n        for quiz in last_5_quizzes:\n            # Track basic metrics progression\n            trends['score_progression'].append(quiz['score'])\n            trends['accuracy_progression'].append(float(quiz['accuracy'].replace(' %', '')))\n            trends['speed_progression'].append(float(quiz['speed']))\n            \n            # Track topic mastery\n            topic = quiz['quiz']['topic']\n            accuracy = float(quiz['accuracy'].replace(' %', ''))\n            trends['topic_mastery'][topic].append(accuracy)\n            \n            # Analyze response patterns\n            duration = self._parse_duration(quiz['duration'])\n            questions_attempted = len(quiz['response_map'])\n            avg_time_per_question = duration / questions_attempted if questions_attempted > 0 else 0\n            trends['question_patterns']['response_time_trends'].append(avg_time_per_question)\n            \n            # Track improvement areas\n            if quiz['incorrect_answers'] > 0:\n                trends['question_patterns']['mistake_patterns'][topic] += quiz['incorrect_answers']\n                if quiz['incorrect_answers'] > 2:  # Threshold for identifying improvement areas\n                    trends['question_patterns']['improvement_areas'].add(topic)\n        \n        return trends\n\n    def _parse_duration(self, duration_str: str) -> float:\n        \"\"\"Convert duration string to minutes\"\"\"\n        try:\n            minutes, seconds = map(int, duration_str.split(':'))\n            return minutes + seconds/60\n        except:\n            return 0\n\n    def calculate_performance_metrics(self) -> Dict:\n        \"\"\"Calculate detailed performance metrics from historical data\"\"\"\n        trends = self.analyze_historical_trends()\n        \n        metrics = {\n            'score': {\n                'trend': self._calculate_trend(trends['score_progression']),\n                'volatility': np.std(trends['score_progression']),\n                'improvement_rate': self._calculate_improvement_rate(trends['score_progression'])\n            },\n            'accuracy': {\n                'trend': self._calculate_trend(trends['accuracy_progression']),\n                'consistency': np.std(trends['accuracy_progression']),\n                'peak_performance': max(trends['accuracy_progression'])\n            },\n            'topic_proficiency': {},\n            'time_management': {\n                'avg_time_per_question': np.mean(trends['question_patterns']['response_time_trends']),\n                'time_trend': self._calculate_trend(trends['question_patterns']['response_time_trends'])\n            }\n        }\n        \n        # Calculate topic proficiency\n        for topic, accuracies in trends['topic_mastery'].items():\n            metrics['topic_proficiency'][topic] = {\n                'average': np.mean(accuracies),\n                'trend': self._calculate_trend(accuracies),\n                'mastery_level': self._determine_mastery_level(accuracies)\n            }\n        \n        return metrics\n\n    def _calculate_trend(self, values: List[float]) -> str:\n        \"\"\"Calculate trend direction and magnitude\"\"\"\n        if len(values) < 2:\n            return \"insufficient data\"\n        \n        slope = (values[0] - values[-1]) / len(values)\n        \n        if abs(slope) < 0.5:\n            return \"stable\"\n        elif slope > 0:\n            return \"improving\" if slope > 2 else \"slightly improving\"\n        else:\n            return \"declining\" if slope < -2 else \"slightly declining\"\n\n    def _calculate_improvement_rate(self, values: List[float]) -> float:\n        \"\"\"Calculate rate of improvement\"\"\"\n        if len(values) < 2:\n            return 0\n        return ((values[0] - values[-1]) / values[-1]) * 100\n\n    def _determine_mastery_level(self, accuracies: List[float]) -> str:\n        \"\"\"Determine mastery level based on accuracy trends\"\"\"\n        avg_accuracy = np.mean(accuracies)\n        consistency = np.std(accuracies)\n        \n        if avg_accuracy >= 90 and consistency < 5:\n            return \"mastered\"\n        elif avg_accuracy >= 80:\n            return \"proficient\"\n        elif avg_accuracy >= 70:\n            return \"developing\"\n        else:\n            return \"needs improvement\"\n\n    def generate_detailed_report(self) -> Dict:\n        \"\"\"Generate a detailed performance report\"\"\"\n        metrics = self.calculate_performance_metrics()\n        trends = self.analyze_historical_trends()\n        \n        report = {\n            'performance_summary': {\n                'overall_trend': metrics['score']['trend'],\n                'consistency_level': 'high' if metrics['score']['volatility'] < 10 else 'moderate' if metrics['score']['volatility'] < 20 else 'low',\n                'improvement_rate': f\"{metrics['score']['improvement_rate']:.1f}%\"\n            },\n            'topic_analysis': {\n                topic: {\n                    'mastery_level': data['mastery_level'],\n                    'trend': data['trend'],\n                    'recommendation': self._generate_topic_recommendation(data)\n                }\n                for topic, data in metrics['topic_proficiency'].items()\n            },\n            'time_management': {\n                'efficiency': metrics['time_management']['time_trend'],\n                'avg_time_per_question': f\"{metrics['time_management']['avg_time_per_question']:.1f} minutes\",\n                'recommendation': self._generate_time_recommendation(metrics['time_management'])\n            },\n            'improvement_areas': list(trends['question_patterns']['improvement_areas']),\n            'strengths': [\n                topic for topic, data in metrics['topic_proficiency'].items()\n                if data['mastery_level'] in ['mastered', 'proficient']\n            ]\n        }\n        \n        return report\n\n    def _generate_topic_recommendation(self, topic_data: Dict) -> str:\n        \"\"\"Generate specific recommendations based on topic performance\"\"\"\n        if topic_data['mastery_level'] == 'needs improvement':\n            return \"Focus on fundamental concepts and increase practice frequency\"\n        elif topic_data['mastery_level'] == 'developing':\n            return \"Continue regular practice and focus on weak areas\"\n        elif topic_data['mastery_level'] == 'proficient':\n            return \"Maintain current performance and work on advanced concepts\"\n        else:\n            return \"Focus on maintaining mastery and helping others\"\n\n    def _generate_time_recommendation(self, time_data: Dict) -> str:\n        \"\"\"Generate time management recommendations\"\"\"\n        avg_time = time_data['avg_time_per_question']\n        if avg_time > 2:\n            return \"Work on improving question solving speed\"\n        elif time_data['time_trend'] == 'declining':\n            return \"Focus on maintaining accuracy while increasing speed\"\n        else:\n            return \"Current time management is effective\"\n\n    def generate_performance_visualizations(self):\n        \"\"\"Generate comprehensive performance visualizations\"\"\"\n        plt.style.use('seaborn')\n        \n        # Create a figure with subplots\n        fig = plt.figure(figsize=(20, 25))\n        gs = GridSpec(4, 2, figure=fig)\n        \n        # Add a main title\n        fig.suptitle('Student Performance Analysis Dashboard', fontsize=16, y=0.95)\n        \n        # 1. Topic Performance Bar Chart\n        ax1 = fig.add_subplot(gs[0, :])\n        self._plot_topic_performance(ax1)\n        \n        # 2. Score Progression Line Chart\n        ax2 = fig.add_subplot(gs[1, 0])\n        self._plot_score_progression(ax2)\n        \n        # 3. Accuracy vs Speed Scatter Plot\n        ax3 = fig.add_subplot(gs[1, 1])\n        self._plot_accuracy_speed_relationship(ax3)\n        \n        # 4. Error Types Distribution\n        ax4 = fig.add_subplot(gs[2, 0])\n        self._plot_error_distribution(ax4)\n        \n        # 5. Question Response Pattern\n        ax5 = fig.add_subplot(gs[2, 1])\n        self._plot_response_pattern(ax5)\n        \n        # 6. Performance Metrics Summary\n        ax6 = fig.add_subplot(gs[3, :])\n        self._plot_performance_metrics(ax6)\n        \n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig('performance_analysis.png', dpi=300, bbox_inches='tight')\n        plt.close()\n\n    def _plot_topic_performance(self, ax):\n        \"\"\"Plot topic-wise performance bar chart\"\"\"\n        topic_perf = self.analyze_performance()['topic_performance']\n        topics = list(topic_perf.keys())\n        scores = list(topic_perf.values())\n        \n        # Create bars\n        bars = ax.bar(topics, scores)\n        \n        # Customize bars\n        for i, bar in enumerate(bars):\n            if scores[i] >= 80:\n                bar.set_color('#2ecc71')  # Green for good performance\n            elif scores[i] >= 60:\n                bar.set_color('#f1c40f')  # Yellow for average\n            else:\n                bar.set_color('#e74c3c')  # Red for needs improvement\n            \n            # Add value labels on top of bars\n            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n                    f'{scores[i]:.1f}%',\n                    ha='center', va='bottom')\n        \n        ax.set_title('Topic-wise Performance Analysis', fontsize=12, pad=20)\n        ax.set_xlabel('Topics', fontsize=10)\n        ax.set_ylabel('Accuracy (%)', fontsize=10)\n        ax.tick_params(axis='x', rotation=45)\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 100)\n\n    def _plot_score_progression(self, ax):\n        \"\"\"Plot score progression over time\"\"\"\n        scores = [quiz['score'] for quiz in self.historical_data[:5]]\n        dates = [quiz['submitted_at'].split('T')[0] for quiz in self.historical_data[:5]]\n        \n        # Plot line with markers\n        ax.plot(dates, scores, 'o-', color='#3498db', linewidth=2, markersize=8)\n        \n        # Add value labels\n        for i, score in enumerate(scores):\n            ax.text(i, score, f'{score:.1f}', ha='center', va='bottom')\n        \n        ax.set_title('Score Progression Over Time', fontsize=12, pad=20)\n        ax.set_xlabel('Date', fontsize=10)\n        ax.set_ylabel('Score', fontsize=10)\n        ax.tick_params(axis='x', rotation=45)\n        ax.grid(True, alpha=0.3)\n\n    def _plot_accuracy_speed_relationship(self, ax):\n        \"\"\"Plot accuracy vs speed scatter plot\"\"\"\n        accuracies = [float(quiz['accuracy'].replace(' %', '')) \n                     for quiz in self.historical_data[:5]]\n        speeds = [float(quiz['speed']) for quiz in self.historical_data[:5]]\n        \n        # Create scatter plot\n        scatter = ax.scatter(speeds, accuracies, c=range(len(speeds)), \n                            cmap='viridis', s=100)\n        \n        # Add trend line\n        z = np.polyfit(speeds, accuracies, 1)\n        p = np.poly1d(z)\n        ax.plot(speeds, p(speeds), \"r--\", alpha=0.8, label='Trend')\n        \n        ax.set_title('Speed vs Accuracy Relationship', fontsize=12, pad=20)\n        ax.set_xlabel('Speed', fontsize=10)\n        ax.set_ylabel('Accuracy (%)', fontsize=10)\n        ax.grid(True, alpha=0.3)\n        ax.legend()\n\n    def _plot_error_distribution(self, ax):\n        \"\"\"Plot error types distribution\"\"\"\n        mistakes = self.analyze_question_mistakes()\n        \n        error_types = ['Conceptual', 'Calculation', 'Comprehension']\n        error_counts = [\n            len(mistakes['conceptual_errors']),\n            len(mistakes['calculation_errors']),\n            len(mistakes['comprehension_errors'])\n        ]\n        \n        colors = ['#e74c3c', '#f39c12', '#3498db']\n        wedges, texts, autotexts = ax.pie(error_counts, labels=error_types, colors=colors,\n                                         autopct='%1.1f%%', startangle=90)\n        \n        # Enhance the appearance\n        plt.setp(autotexts, size=8, weight=\"bold\")\n        plt.setp(texts, size=10)\n        \n        ax.set_title('Distribution of Error Types', fontsize=12, pad=20)\n\n    def _plot_response_pattern(self, ax):\n        \"\"\"Plot question response pattern analysis\"\"\"\n        patterns = self.analyze_question_patterns()\n        \n        # Prepare data\n        categories = ['Correct', 'Time-based Errors', 'Careless Mistakes']\n        values = [\n            self.current_quiz['correct_answers'],\n            patterns['time_based_errors'],\n            patterns['careless_mistakes']\n        ]\n        \n        # Create horizontal bars\n        bars = ax.barh(categories, values, color=['#2ecc71', '#e74c3c', '#f1c40f'])\n        \n        # Add value labels\n        for bar in bars:\n            width = bar.get_width()\n            ax.text(width, bar.get_y() + bar.get_height()/2.,\n                    f'{int(width)}', \n                    ha='left', va='center', fontsize=10)\n        \n        ax.set_title('Question Response Pattern', fontsize=12, pad=20)\n        ax.set_xlabel('Number of Questions', fontsize=10)\n        ax.grid(True, alpha=0.3)\n\n    def _plot_performance_metrics(self, ax):\n        \"\"\"Plot performance metrics summary\"\"\"\n        metrics = self.calculate_performance_metrics()\n        \n        # Prepare data\n        categories = ['Score Trend', 'Accuracy', 'Time Management', 'Consistency']\n        values = [\n            float(metrics['score']['improvement_rate']),\n            metrics['accuracy']['peak_performance'],\n            100 - (metrics['time_management']['avg_time_per_question'] * 10),  # Convert to percentage\n            100 - (metrics['score']['volatility'] * 2)  # Convert to percentage\n        ]\n        \n        # Create radar chart\n        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)\n        values = np.concatenate((values, [values[0]]))  # complete the loop\n        angles = np.concatenate((angles, [angles[0]]))  # complete the loop\n        \n        ax.plot(angles, values, 'o-', linewidth=2)\n        ax.fill(angles, values, alpha=0.25)\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels(categories)\n        \n        ax.set_title('Performance Metrics Summary', fontsize=12, pad=20)\n        ax.grid(True)\n\n    def analyze_question_mistakes(self) -> Dict:\n        \"\"\"Analyze incorrect answers in detail\"\"\"\n        mistakes_analysis = {\n            'conceptual_errors': [],\n            'calculation_errors': [],\n            'comprehension_errors': [],\n            'topic_wise_mistakes': defaultdict(list),\n            'pattern_summary': ''\n        }\n        \n        # Analyze current quiz responses\n        for q_id, opt_id in self.current_quiz['response_map'].items():\n            question = next((q for q in self.quiz_questions['quiz']['questions'] \n                           if str(q['id']) == str(q_id)), None)\n            \n            if question:\n                correct_option = next((opt for opt in question['options'] \n                                    if opt['is_correct']), None)\n                selected_option = next((opt for opt in question['options'] \n                                     if str(opt['id']) == str(opt_id)), None)\n                \n                if correct_option and selected_option and str(correct_option['id']) != str(opt_id):\n                    mistake = {\n                        'question': question['description'],\n                        'correct_answer': correct_option['description'],\n                        'student_answer': selected_option['description'],\n                        'topic': question['topic'],\n                        'detailed_solution': question.get('detailed_solution', '')  # Use get() with default value\n                    }\n                    \n                    # Analyze mistake type using solution if available\n                    solution_text = mistake['detailed_solution'].lower() if mistake['detailed_solution'] else ''\n                    \n                    if 'calculation' in solution_text or any(word in question['description'].lower() \n                       for word in ['calculate', 'compute', 'find', 'solve']):\n                        mistakes_analysis['calculation_errors'].append(mistake)\n                    elif solution_text and any(word in solution_text \n                         for word in ['concept', 'principle', 'theory']):\n                        mistakes_analysis['conceptual_errors'].append(mistake)\n                    else:\n                        mistakes_analysis['comprehension_errors'].append(mistake)\n                    \n                    mistakes_analysis['topic_wise_mistakes'][question['topic']].append(mistake)\n        \n        return mistakes_analysis\n\n    def _format_mistakes_for_prompt(self, mistakes_analysis: Dict) -> str:\n        \"\"\"Format mistakes analysis into a structured string for the prompt\"\"\"\n        formatted_mistakes = \"Mistakes Analysis:\\n\"\n        \n        # Add topic-wise mistakes\n        for topic, mistakes in mistakes_analysis['topic_wise_mistakes'].items():\n            formatted_mistakes += f\"\\nTopic: {topic}\\n\"\n            for idx, mistake in enumerate(mistakes, 1):\n                formatted_mistakes += f\"{idx}. Question: {mistake['question']}\\n\"\n                formatted_mistakes += f\"   - Student Answer: {mistake['student_answer']}\\n\"\n                formatted_mistakes += f\"   - Correct Answer: {mistake['correct_answer']}\\n\"\n        \n        return formatted_mistakes\n\n    def _generate_llm_analysis(self, prompt: str) -> str:\n        \"\"\"Generate analysis using LLM\"\"\"\n        try:\n            # Format prompt\n            formatted_prompt = f\"\"\"<s>[INST] <<SYS>>\nYou are an expert educational advisor specializing in NEET exam preparation.\nYour task is to analyze student performance and provide detailed, actionable recommendations.\n<</SYS>>\n\n{prompt} [/INST]\"\"\"\n            \n            # Generate response\n            outputs = self.pipe(\n                formatted_prompt,\n                max_new_tokens=1024,\n                temperature=0.7,\n                top_p=0.9,\n                repetition_penalty=1.1,\n                do_sample=True,\n                return_full_text=False\n            )\n            \n            if outputs and len(outputs) > 0:\n                response = outputs[0]['generated_text']\n                # Clean up response\n                response = response.replace(\"[/INST]\", \"\").strip()\n                return response\n            \n            return \"Error: Unable to generate analysis\"\n            \n        except Exception as e:\n            print(f\"Error in LLM analysis: {str(e)}\")\n            return f\"Error generating analysis: {str(e)}\"\n\n    def _generate_llm_prompt(self, analysis: Dict, mistakes: Dict) -> str:\n        \"\"\"Generate detailed prompt for Llama\"\"\"\n        prompt = f\"\"\"Task: Analyze student performance in NEET preparation and create a personalized study plan.\n\nPERFORMANCE ANALYSIS:\nScore: {analysis['average_score']:.1f}/100\nAccuracy: {analysis['average_accuracy']:.1f}%\nSpeed: {analysis['average_speed']:.1f}%\n\nSTRENGTHS:\n{', '.join(analysis['strengths']) if analysis['strengths'] else 'No clear strengths identified yet'}\n\nWEAK AREAS:\n{', '.join(analysis['weaknesses']) if analysis['weaknesses'] else 'No significant weaknesses identified'}\n\nERROR ANALYSIS:\n‚Ä¢ Conceptual Errors: {len(mistakes['conceptual_errors'])}\n‚Ä¢ Calculation Errors: {len(mistakes['calculation_errors'])}\n‚Ä¢ Understanding Errors: {len(mistakes['comprehension_errors'])}\n\nSPECIFIC MISTAKES:\n\"\"\"\n        # Add detailed mistake examples\n        for topic, topic_mistakes in mistakes['topic_wise_mistakes'].items():\n            if topic_mistakes:\n                prompt += f\"\\n{topic}:\\n\"\n                for mistake in topic_mistakes[:2]:\n                    prompt += f\"\"\"Question: {mistake['question']}\n- Student's Answer: {mistake['student_answer']}\n- Correct Answer: {mistake['correct_answer']}\n- Concept: {mistake['detailed_solution'][:150]}...\\n\"\"\"\n\n        prompt += \"\"\"\nBased on this analysis, provide:\n\n1. IMMEDIATE ACTIONS (3-4 specific steps):\n- What should the student focus on right now?\n- Which concepts need urgent revision?\n\n2. WEEKLY STUDY PLAN:\n- Day-by-day schedule\n- Time allocation for each topic\n- Specific resources to use\n\n3. TOPIC-WISE STRATEGY:\n- For each weak area, provide:\n  * Key concepts to master\n  * Recommended practice approach\n  * Common pitfalls to avoid\n\n4. PRACTICE RECOMMENDATIONS:\n- Types of questions to focus on\n- Time management tips\n- Error prevention strategies\n\nFormat your response with clear headings and bullet points. Be specific and actionable.\"\"\"\n\n        return prompt\n\n    def analyze_mistakes_with_llm(self) -> str:\n        \"\"\"Generate detailed mistake analysis using Llama\"\"\"\n        mistakes = self.analyze_question_mistakes()\n        \n        prompt = f\"\"\"Task: Analyze student's mistakes in NEET preparation and provide detailed feedback.\n\nMISTAKE PATTERNS:\n‚Ä¢ Conceptual Errors: {len(mistakes['conceptual_errors'])}\n‚Ä¢ Calculation Errors: {len(mistakes['calculation_errors'])}\n‚Ä¢ Understanding Errors: {len(mistakes['comprehension_errors'])}\n\nDETAILED EXAMPLES:\"\"\"\n        \n        for category, errors in [\n            (\"Conceptual\", mistakes['conceptual_errors']),\n            (\"Calculation\", mistakes['calculation_errors']),\n            (\"Understanding\", mistakes['comprehension_errors'])\n        ]:\n            if errors:\n                prompt += f\"\\n\\n{category} Mistakes:\\n\"\n                for error in errors[:2]:\n                    prompt += f\"\"\"\nQuestion: {error['question']}\nStudent's Answer: {error['student_answer']}\nCorrect Answer: {error['correct_answer']}\nTopic: {error['topic']}\nExplanation: {error['detailed_solution'][:200]}...\\n\"\"\"\n\n        prompt += \"\"\"\nProvide a detailed analysis including:\n\n1. PATTERN ANALYSIS:\n- What are the common themes in these mistakes?\n- Are there specific topics or concepts that need attention?\n\n2. CONCEPTUAL GAPS:\n- Identify fundamental concepts that need strengthening\n- Explain how these gaps affect performance\n\n3. IMPROVEMENT STRATEGY:\n- Specific steps to address each type of error\n- Study techniques to prevent similar mistakes\n- Practice recommendations\n\n4. QUICK WINS:\n- Immediate actions to improve performance\n- Common traps to avoid\n- Time management tips\n\nBe specific and provide actionable advice that the student can implement immediately.\"\"\"\n\n        return self._generate_llm_analysis(prompt)\n\n    def generate_study_plan(self) -> Dict:\n        \"\"\"Generate personalized study plan using LLM\"\"\"\n        analysis = self.analyze_performance()\n        mistakes = self.analyze_question_mistakes()\n        \n        # Generate prompt\n        prompt = self._generate_llm_prompt(analysis, mistakes)\n        \n        # Get LLM response\n        llm_response = self._generate_llm_analysis(prompt)\n        \n        # Parse the response into structured format\n        study_plan = self._parse_study_plan(llm_response)\n        \n        return study_plan\n\n    def _parse_study_plan(self, llm_response: str) -> Dict:\n        \"\"\"Parse LLM response into structured study plan\"\"\"\n        study_plan = {\n            'immediate_focus': [],\n            'weekly_goals': [],\n            'topic_wise_preparation': {},\n            'practice_recommendations': [],\n            'recommended_resources': []\n        }\n        \n        current_section = None\n        current_topic = None\n        \n        # Split response into lines and process\n        for line in llm_response.split('\\n'):\n            line = line.strip()\n            if not line:\n                continue\n            \n            # Identify sections\n            if 'IMMEDIATE ACTIONS' in line:\n                current_section = 'immediate_focus'\n            elif 'WEEKLY STUDY PLAN' in line:\n                current_section = 'weekly_goals'\n            elif 'TOPIC-WISE STRATEGY' in line:\n                current_section = 'topic_wise_preparation'\n            elif 'PRACTICE RECOMMENDATIONS' in line:\n                current_section = 'practice_recommendations'\n            elif line.endswith(':') and current_section == 'topic_wise_preparation':\n                current_topic = line[:-1]\n                study_plan['topic_wise_preparation'][current_topic] = []\n            elif line.startswith('- ') or line.startswith('* '):\n                content = line[2:].strip()\n                if current_section == 'topic_wise_preparation' and current_topic:\n                    study_plan['topic_wise_preparation'][current_topic].append(content)\n                elif current_section:\n                    study_plan[current_section].append(content)\n        \n        return study_plan\n\n    def analyze_and_generate_report(self) -> Dict:\n        \"\"\"Pipeline to generate complete analysis and recommendations\"\"\"\n        try:\n            # Initialize report structure\n            report = {\n                'performance_analysis': None,\n                'detailed_report': None,\n                'student_persona': None,\n                'mistakes_analysis': None,\n                'study_plan': None,\n                'llm_analysis': None\n            }\n            \n            # Step 1: Basic Performance Analysis\n            report['performance_analysis'] = self.analyze_performance()\n            if not report['performance_analysis']:\n                raise ValueError(\"Failed to generate performance analysis\")\n            \n            # Step 2: Mistake Analysis\n            report['mistakes_analysis'] = self.analyze_question_mistakes()\n            if not report['mistakes_analysis']:\n                raise ValueError(\"Failed to generate mistake analysis\")\n            \n            # Step 3: Generate Detailed Report\n            report['detailed_report'] = self.generate_detailed_report()\n            if not report['detailed_report']:\n                raise ValueError(\"Failed to generate detailed report\")\n            \n            # Step 4: Get Student Persona\n            report['student_persona'] = self.get_student_persona()\n            if not report['student_persona']:\n                raise ValueError(\"Failed to generate student persona\")\n            \n            # Step 5: Generate Study Plan\n            report['study_plan'] = self.generate_study_plan()\n            \n            # Step 6: LLM-based Mistake Analysis\n            report['llm_analysis'] = self.analyze_mistakes_with_llm()\n            \n            # Step 7: Generate Visualizations\n            try:\n                self.generate_performance_visualizations()\n            except Exception as viz_error:\n                print(f\"Warning: Failed to generate visualizations: {str(viz_error)}\")\n            \n            return report\n            \n        except Exception as e:\n            print(f\"Error in analysis pipeline: {str(e)}\")\n            return None\n\n    def _clean_llm_response(self, response: str) -> str:\n        \"\"\"Clean and format LLM response for better readability\"\"\"\n        # Remove any prompt artifacts\n        response = response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\")\n        response = response.replace(\"Context:\", \"\").replace(\"Question:\", \"\")\n        \n        # Get only the relevant part of the response\n        if \"Provide a clear, concise response\" in response:\n            response = response.split(\"Provide a clear, concise response\")[0]\n        \n        # Clean up extra whitespace and newlines\n        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n        response = '\\n'.join(lines)\n        \n        return response\n\n    def _format_qa_prompt(self, question: str, context: Dict) -> str:\n        \"\"\"Format prompt for Q&A to get more structured responses\"\"\"\n        return f\"\"\"[INST] You are a NEET exam preparation expert. Provide a clear, structured response to the student's question.\n\nCurrent Performance:\n‚Ä¢ Score: {context['performance']['average_score']:.1f}\n‚Ä¢ Accuracy: {context['performance']['average_accuracy']:.1f}%\n‚Ä¢ Areas for improvement: {', '.join(context['performance']['weaknesses'])}\n\nStudent's Question: {question}\n\nProvide your response in this format:\n1. Direct Answer\n2. Explanation (if needed)\n3. Specific Tips or Actions\n4. Related Topics to Study\n\nKeep each section concise and focused. [/INST]\"\"\"\n\ndef main():\n    try:\n        analyzer = StudentAnalyzer()\n        \n        # Get all analyses first\n        performance = analyzer.analyze_performance()\n        mistakes = analyzer.analyze_question_mistakes()\n        patterns = analyzer.analyze_question_patterns()\n        \n        # Print sections with clear separation\n        print(\"\\n\" + \"=\"*50)\n        print(\"üìä PERFORMANCE SUMMARY\".center(50))\n        print(\"=\"*50)\n        print(f\"üìà Average Score:    {performance['average_score']:.1f}\")\n        print(f\"üéØ Average Accuracy: {performance['average_accuracy']:.1f}%\")\n        print(f\"‚ö° Average Speed:    {performance['average_speed']:.1f}%\")\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"üìö TOPIC ANALYSIS\".center(50))\n        print(\"=\"*50)\n        print(\"\\nüí™ Strong Topics:\")\n        for topic in performance['strengths']:\n            accuracy = performance['topic_performance'].get(topic, 0)\n            print(f\"  ‚úì {topic}: {accuracy:.1f}%\")\n            \n        print(\"\\nüìù Topics Needing Improvement:\")\n        for topic in performance['weaknesses']:\n            accuracy = performance['topic_performance'].get(topic, 0)\n            print(f\"  ‚Ä¢ {topic}: {accuracy:.1f}%\")\n        \n        print(\"\\n\" + \"=\"*50)\n        print(\"‚ùå MISTAKE ANALYSIS\".center(50))\n        print(\"=\"*50)\n        print(f\"\\nQuestions Attempted: {len(analyzer.current_quiz['response_map'])}\")\n        print(f\"‚úì Correct Answers:   {analyzer.current_quiz['correct_answers']}\")\n        print(f\"‚úó Incorrect Answers: {analyzer.current_quiz['incorrect_answers']}\")\n        \n        # Show incorrect answers in a cleaner format\n        if mistakes['topic_wise_mistakes']:\n            print(\"\\n\" + \"-\"*50)\n            print(\"Detailed Analysis of Incorrect Answers:\")\n            print(\"-\"*50)\n            for topic, mistakes_list in mistakes['topic_wise_mistakes'].items():\n                print(f\"\\nüìò Topic: {topic}\")\n                for idx, mistake in enumerate(mistakes_list, 1):\n                    print(f\"\\nQuestion {idx}:\")\n                    print(f\"Q: {mistake['question']}\")\n                    print(f\"‚úó Your Answer:     {mistake['student_answer']}\")\n                    print(f\"‚úì Correct Answer:  {mistake['correct_answer']}\")\n                    if mistake['detailed_solution']:\n                        print(f\"üí° Explanation: {mistake['detailed_solution']}\")\n                    print(\"-\"*40)\n        \n        # Generate visualizations\n        analyzer.generate_performance_visualizations()\n        print(\"\\nüìä Visualization plots saved as 'performance_analysis.png'\")\n        \n        # AI Analysis section\n        print(\"\\n\" + \"=\"*50)\n        print(\"ü§ñ AI-POWERED RECOMMENDATIONS\".center(50))\n        print(\"=\"*50)\n        \n        # Format mistakes for LLM\n        formatted_mistakes = analyzer._format_mistakes_for_prompt(mistakes)\n        analysis_prompt = f\"\"\"\nBased on the student's performance:\n- Score: {performance['average_score']:.1f}\n- Accuracy: {performance['average_accuracy']:.1f}%\n- Speed: {performance['average_speed']:.1f}%\n\n{formatted_mistakes}\n\nPlease provide a concise analysis with:\n1. Key misconceptions identified\n2. Specific topics to review\n3. Recommended study approach\n4. Practice suggestions\n\"\"\"\n        # Get and print LLM analysis\n        response = analyzer.pipe(\n            analysis_prompt,\n            max_new_tokens=512,  # Reduced for conciseness\n            temperature=0.7,\n            top_p=0.9,\n            repetition_penalty=1.1,\n            do_sample=True,\n        )\n        print(\"\\n\" + response[0]['generated_text'].strip())\n        \n        # Interactive Q&A session\n        print(\"\\n\" + \"=\"*50)\n        print(\"üí¨ INTERACTIVE LEARNING ASSISTANT\".center(50))\n        print(\"=\"*50)\n        print(\"\\nAsk questions about your performance or concepts (type 'exit' to end)\")\n        print(\"-\"*50)\n        \n        context = {\n            'performance': performance,\n            'mistakes': mistakes,\n            'patterns': patterns\n        }\n        \n        while True:\n            user_input = input(\"\\n‚ùì Your question: \").strip()\n            \n            if user_input.lower() == 'exit':\n                print(\"\\nThank you for using the Learning Assistant! Good luck with your studies! üëã\")\n                break\n            \n            # Get LLM response with formatted prompt\n            response = analyzer.pipe(\n                analyzer._format_qa_prompt(user_input, context),\n                max_new_tokens=256,\n                temperature=0.7,\n                top_p=0.9,\n                repetition_penalty=1.1,\n                do_sample=True,\n            )\n            \n            # Clean and print response\n            cleaned_response = analyzer._clean_llm_response(response[0]['generated_text'])\n            \n            print(\"\\nüí° Answer:\")\n            print(\"-\"*50)\n            \n            # Format the sections\n            sections = cleaned_response.split('\\n')\n            current_section = \"\"\n            \n            for line in sections:\n                if line.strip():\n                    if line.startswith(('1.', '2.', '3.', '4.')):\n                        if current_section:  # Add spacing between sections\n                            print()\n                        current_section = line\n                        print(f\"‚û§ {line.split('.', 1)[1].strip()}\")\n                    else:\n                        print(f\"  {line.strip()}\")\n            \n            print(\"-\"*50)\n        \n    except Exception as e:\n        print(f\"‚ùå Error in analysis: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T14:19:13.556543Z","iopub.execute_input":"2025-01-25T14:19:13.557025Z","iopub.status.idle":"2025-01-25T14:33:32.124961Z","shell.execute_reply.started":"2025-01-25T14:19:13.556974Z","shell.execute_reply":"2025-01-25T14:33:32.124109Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading LLM model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9638f430fe44041bed37cf10819bad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5826ff25afa94243a12c58d0183c6731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df829974d9f1437eb4e6cefcd1dccaf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1b4c5c724e416f87cc10e0f6bcc95a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94e85c09db34bf28b0b45a2ad895742"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50716b4f00fe44f2ab1b93f4bb1f6a5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09703189443a4bbca3f293d95eaf3f19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9033081b23624836a239bd7f5deadb37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ca1b431358940f9a8447f495fcf3475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65f99eca74584000bfd6b043b7d4b25b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n<ipython-input-1-018193becf0e>:478: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn')\n","output_type":"stream"},{"name":"stdout","text":"LLM model loaded successfully!\n\n==================================================\n              üìä PERFORMANCE SUMMARY               \n==================================================\nüìà Average Score:    77.6\nüéØ Average Accuracy: 81.4%\n‚ö° Average Speed:    99.2%\n\n==================================================\n                 üìö TOPIC ANALYSIS                 \n==================================================\n\nüí™ Strong Topics:\n  ‚úì Body Fluids and Circulation : 95.0%\n\nüìù Topics Needing Improvement:\n  ‚Ä¢ Body Fluids and Circulation: 72.3%\n\n==================================================\n                ‚ùå MISTAKE ANALYSIS                \n==================================================\n\nQuestions Attempted: 10\n‚úì Correct Answers:   8\n‚úó Incorrect Answers: 2\n\n--------------------------------------------------\nDetailed Analysis of Incorrect Answers:\n--------------------------------------------------\n\nüìò Topic: structural organisation in animals \n\nQuestion 1:\nQ: The secretions of endocrine glands are released directly\n‚úó Your Answer:     into the brain tissue\n‚úì Correct Answer:  into the blood stream\n----------------------------------------\n\nQuestion 2:\nQ: Vasa efferentia in male frog, enter the kidney and open into\n‚úó Your Answer:     Urinogenital duct\n‚úì Correct Answer:  Bidder's canal\n----------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Visualization plots saved as 'performance_analysis.png'\n\n==================================================\n           ü§ñ AI-POWERED RECOMMENDATIONS           \n==================================================\n\nBased on the student's performance:\n- Score: 77.6\n- Accuracy: 81.4%\n- Speed: 99.2%\n\nMistakes Analysis:\n\nTopic: structural organisation in animals \n1. Question: The secretions of endocrine glands are released directly\n   - Student Answer: into the brain tissue\n   - Correct Answer: into the blood stream\n2. Question: Vasa efferentia in male frog, enter the kidney and open into\n   - Student Answer: Urinogenital duct\n   - Correct Answer: Bidder's canal\n\n\nPlease provide a concise analysis with:\n1. Key misconceptions identified\n2. Specific topics to review\n3. Recommended study approach\n4. Practice suggestions\n5. Evaluation criteria for future assessment\n\nAnalysis:\n\nKey Misconceptions Identified:\n1. Structural Organisation in Animals - Endocrine Glands Secretion (students think secretion goes directly to the brain)\n   - This is incorrect as the correct answer states that endocrine glands secrete their hormones directly into the bloodstream.\n\n2. Vasa Efferentia in Male Frog - Kidney Function (student thinks it opens into the Urinogenital duct)\n   - This is also incorrect as vasa efferentia actually connects to Bidder's canal.\n\nSpecific Topics to Review:\n- Structure and function of endocrine glands\n- Vasa efferentia and its role in the urinary system\n\nRecommended Study Approach:\n- Focus on understanding the basic functions of endocrine glands.\n- Review diagrams illustrating vasa efferentia and its connections to kidneys in amphibians.\n\nPractice Suggestions:\n- Use flashcards or concept maps to reinforce key terms related to endocrine glands and vasa efferentia.\n- Label diagrams of the frog's urinary system to better understand the connection between vasa efferentia and Bidder's canal.\n\nEvaluation Criteria for Future Assessment:\n- Demonstrate a clear understanding of how endocrine glands produce hormones and release them into the bloodstream.\n- Show an accurate representation of vasa efferentia and its role in the urinary system in frogs.\n\nAdditional Recommendations:\n- Consider incorporating interactive activities such as games or quizzes to help students engage with complex biological concepts like endocrine systems.\n- Provide opportunities for students to collaborate and discuss challenging topics, fostering a deeper understanding of the material.\n\n==================================================\n         üí¨ INTERACTIVE LEARNING ASSISTANT         \n==================================================\n\nAsk questions about your performance or concepts (type 'exit' to end)\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n‚ùì Your question:  what topics should i study now?\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nüí° Answer:\n--------------------------------------------------\n  You are a NEET exam preparation expert. Provide a clear, structured response to the student's question.\n  Current Performance:\n  ‚Ä¢ Score: 77.6\n  ‚Ä¢ Accuracy: 81.4%\n  ‚Ä¢ Areas for improvement: Body Fluids and Circulation\n  Student's  what topics should i study now?\n  Provide your response in this format:\n‚û§ Direct Answer\n\n‚û§ Explanation (if needed)\n\n‚û§ Specific Tips or Actions\n\n‚û§ Related Topics to Study\n  Keep each section concise and focused.\n  **Response**\n  **1. Direct Answer**\n  To improve your performance, focus on studying the following topics:\n  - Blood and its components\n  - Blood circulation system\n  - Lymphatic system\n  **2. Explanation**\n  These topics are crucial because they directly relate to your identified areas of weakness in Body Fluids and Circulation. Mastering these concepts will help you better understand how blood and lymph circulate through the body, allowing you to make more accurate connections between different bodily systems.\n  **3. Specific Tips or Actions**\n  - Review diagrams and illustrations to visualize the blood and lymphatic systems.\n  - Practice identifying key structures and functions within these systems.\n  - Focus on understanding the differences between arteries, veins, and capillaries.\n  **4. Related Topics to Study**\n  Additionally, consider reviewing the following related topics to gain a deeper understanding of the interconnectedness of the human body:\n  - Muscular System: Understanding muscle contractions and relaxations is essential for comprehending blood flow and circulation.\n  - Nervous System: The autonomic nervous system plays a significant role in regulating blood pressure and heart rate.\n  - Skeletal System: Familiarize yourself with bone structure and function to appreciate the relationship between bones and blood vessels.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n‚ùì Your question:  exit\n"},{"name":"stdout","text":"\nThank you for using the Learning Assistant! Good luck with your studies! üëã\n","output_type":"stream"}],"execution_count":1}]}